{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ecda9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from simhash import SimHash\n",
    "from simhash_embed import SimHashEmbed\n",
    "from recordlinkage.datasets import load_febrl2, load_febrl3, load_febrl1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import torch \n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76844a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38e65d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_febrl3()\n",
    "df_processed = df.fillna('')\n",
    "merged_column = df_processed.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "df[\"text\"] = merged_column\n",
    "csv_file_path = '/home/hai/Projects/locality-sensitive-hashing/data/febrl/febrl3_processed_with_text.csv'\n",
    "df.to_csv(csv_file_path, index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6cb06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "def get_ngrams(text, n=N):\n",
    "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
    "    tokens = []\n",
    "    for i in range(len(text) - n + 1):\n",
    "        tokens.append(text[i:i+n])      \n",
    "    return tokens\n",
    "\n",
    "def preprocess(text, n=N):\n",
    "    remove_chars = string.punctuation + '@.'\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(str.maketrans('', '', remove_chars))\n",
    "    tokens = get_ngrams(text, n)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5f8d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Computing SimHash with {N}-grams...\")\n",
    "# start_time = time.time()\n",
    "# df['tokens'] = df['text'].apply(lambda x: preprocess(x, N))\n",
    "# df['simhash'] = df['tokens'].apply(lambda x: SimHash(x))\n",
    "# end_time = time.time()\n",
    "# print(f\"SimHash computation completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b4b0d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-1496-org</th>\n",
       "      <td>mitchell</td>\n",
       "      <td>green</td>\n",
       "      <td>7</td>\n",
       "      <td>wallaby place</td>\n",
       "      <td>delmar</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>2119</td>\n",
       "      <td>sa</td>\n",
       "      <td>19560409</td>\n",
       "      <td>1804974</td>\n",
       "      <td>mitchell green 7 wallaby place delmar clevelan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-552-dup-3</th>\n",
       "      <td>harley</td>\n",
       "      <td>mccarthy</td>\n",
       "      <td>177</td>\n",
       "      <td>pridhamstreet</td>\n",
       "      <td>milton</td>\n",
       "      <td>marsden</td>\n",
       "      <td>3165</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19080419</td>\n",
       "      <td>6089216</td>\n",
       "      <td>harley mccarthy 177 pridhamstreet milton marsd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-988-dup-1</th>\n",
       "      <td>madeline</td>\n",
       "      <td>mason</td>\n",
       "      <td>54</td>\n",
       "      <td>hoseason street</td>\n",
       "      <td>lakefront retrmnt vlge</td>\n",
       "      <td>granville</td>\n",
       "      <td>4881</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19081128</td>\n",
       "      <td>2185997</td>\n",
       "      <td>madeline mason 54 hoseason street lakefront re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1716-dup-1</th>\n",
       "      <td>isabelle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>gundulu place</td>\n",
       "      <td>currin ga</td>\n",
       "      <td>utakarra</td>\n",
       "      <td>2193</td>\n",
       "      <td>wa</td>\n",
       "      <td>19921119</td>\n",
       "      <td>4314184</td>\n",
       "      <td>isabelle  23 gundulu place currin ga utakarra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1213-org</th>\n",
       "      <td>taylor</td>\n",
       "      <td>hathaway</td>\n",
       "      <td>7</td>\n",
       "      <td>yuranigh court</td>\n",
       "      <td>brentwood vlge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4220</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19991207</td>\n",
       "      <td>9144092</td>\n",
       "      <td>taylor hathaway 7 yuranigh court brentwood vlg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               given_name   surname street_number        address_1  \\\n",
       "rec_id                                                               \n",
       "rec-1496-org     mitchell     green             7    wallaby place   \n",
       "rec-552-dup-3      harley  mccarthy           177    pridhamstreet   \n",
       "rec-988-dup-1    madeline     mason            54  hoseason street   \n",
       "rec-1716-dup-1   isabelle       NaN            23    gundulu place   \n",
       "rec-1213-org       taylor  hathaway             7   yuranigh court   \n",
       "\n",
       "                             address_2     suburb postcode state  \\\n",
       "rec_id                                                             \n",
       "rec-1496-org                    delmar  cleveland     2119    sa   \n",
       "rec-552-dup-3                   milton    marsden     3165   nsw   \n",
       "rec-988-dup-1   lakefront retrmnt vlge  granville     4881   nsw   \n",
       "rec-1716-dup-1               currin ga   utakarra     2193    wa   \n",
       "rec-1213-org            brentwood vlge        NaN     4220   nsw   \n",
       "\n",
       "               date_of_birth soc_sec_id  \\\n",
       "rec_id                                    \n",
       "rec-1496-org        19560409    1804974   \n",
       "rec-552-dup-3       19080419    6089216   \n",
       "rec-988-dup-1       19081128    2185997   \n",
       "rec-1716-dup-1      19921119    4314184   \n",
       "rec-1213-org        19991207    9144092   \n",
       "\n",
       "                                                             text  \n",
       "rec_id                                                             \n",
       "rec-1496-org    mitchell green 7 wallaby place delmar clevelan...  \n",
       "rec-552-dup-3   harley mccarthy 177 pridhamstreet milton marsd...  \n",
       "rec-988-dup-1   madeline mason 54 hoseason street lakefront re...  \n",
       "rec-1716-dup-1  isabelle  23 gundulu place currin ga utakarra ...  \n",
       "rec-1213-org    taylor hathaway 7 yuranigh court brentwood vlg...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6677aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(N, N))  \n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# def compute_simhash_with_tfidf(row_vector, feature_names):\n",
    "#     tokens = []\n",
    "#     weights = []\n",
    "#     arr = row_vector.toarray()[0]\n",
    "#     for token, w in zip(feature_names, arr):\n",
    "#         if w > 0:\n",
    "#             tokens.append(token)\n",
    "#             weights.append(w)\n",
    "#     return SimHash(tokens=tokens, weights=weights)\n",
    "\n",
    "# df['simhash'] = [compute_simhash_with_tfidf(row, feature_names) for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58386111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_id\n",
      "rec-1496-org      145596705836023047667209662387269495559\n",
      "rec-552-dup-3     337679108322715786090421873132322221934\n",
      "rec-988-dup-1     126696736955074268544203404394783279686\n",
      "rec-1716-dup-1    273960009746506298631348661249471892324\n",
      "rec-1213-org      101472903131678903313445802249241531238\n",
      "Name: simhash, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"simhash\"] = [SimHashEmbed(text=row_text, f=128, model=model, device=device) for row_text in df[\"text\"]]\n",
    "print(df[\"simhash\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdbc2998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4444 similar pairs within Hamming distance 20.\n",
      "Search completed in 8.26 seconds.\n"
     ]
    }
   ],
   "source": [
    "K = 20\n",
    "def hamming_distance(x, y):\n",
    "    return bin(x ^ y).count(\"1\")\n",
    "def brute_force(df, K=K):\n",
    "    hashes = df['simhash'].tolist()\n",
    "    num_records = len(hashes)\n",
    "    found_pairs = set()\n",
    "\n",
    "    # def hamming_distance(hash1, hash2):\n",
    "    #     x = hash1 ^ hash2\n",
    "    #     return bin(x).count('1')\n",
    "\n",
    "    search_start_time = time.time()\n",
    "    for i in range(num_records):\n",
    "        for j in range(i + 1, num_records):\n",
    "            #dist = hamming_distance(hashes[i], hashes[j])\n",
    "            dist = hashes[i].distance(hashes[j])\n",
    "            if dist <= K:\n",
    "                found_pairs.add(tuple(sorted((i, j))))\n",
    "    search_end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(found_pairs)} similar pairs within Hamming distance {K}.\")\n",
    "    print(f\"Search completed in {search_end_time - search_start_time:.2f} seconds.\")\n",
    "    return found_pairs\n",
    "found_pairs_bf = brute_force(df, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea17ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Ground Truth from IDs...\n",
      "Found 6538 actual duplicate pairs in 0.0066s\n",
      "Ground truth pairs:6538\n",
      "Total candidates found by SimHash:4444\n",
      "True positives:3315\n",
      "Missed:3223\n",
      "Precision:0.7459\n",
      "Recall:0.5070\n",
      "F1-Score:0.6037\n"
     ]
    }
   ],
   "source": [
    "def generate_ground_truth(df):\n",
    "    print(\"Generating Ground Truth from IDs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    entity_groups = {}\n",
    "    \n",
    "    for int_idx, rec_id_str in enumerate(df.index):\n",
    "        try:\n",
    "            parts = rec_id_str.split('-')\n",
    "            entity_id = parts[1]\n",
    "            \n",
    "            if entity_id not in entity_groups:\n",
    "                entity_groups[entity_id] = []\n",
    "            entity_groups[entity_id].append(int_idx)\n",
    "            \n",
    "        except IndexError:\n",
    "            continue \n",
    "            \n",
    "    true_pairs = set()\n",
    "    for ent_id, indices in entity_groups.items():\n",
    "        if len(indices) > 1:\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pair = tuple(sorted((indices[i], indices[j])))\n",
    "                    true_pairs.add(pair)\n",
    "                    \n",
    "    print(f\"Found {len(true_pairs)} actual duplicate pairs in {time.time() - start_time:.4f}s\")\n",
    "    return true_pairs\n",
    "\n",
    "def lsh_candidates(simhashes, band_size, num_bands):\n",
    "    buckets = [{} for _ in range(num_bands)]\n",
    "    candidates = set()\n",
    "\n",
    "    for idx, h_obj in enumerate(simhashes):\n",
    "        h = h_obj.value\n",
    "        for b in range(num_bands):\n",
    "            start = b * band_size\n",
    "            mask = (1 << band_size) - 1\n",
    "            band_value = (h >> start) & mask\n",
    "            if band_value not in buckets[b]:\n",
    "                buckets[b][band_value] = []\n",
    "            buckets[b][band_value].append(idx)\n",
    "\n",
    "    for b in range(num_bands):\n",
    "        for bucket_indices in buckets[b].values():\n",
    "            if len(bucket_indices) > 1:\n",
    "                for i in range(len(bucket_indices)):\n",
    "                    for j in range(i + 1, len(bucket_indices)):\n",
    "                        pair = tuple(sorted((bucket_indices[i], bucket_indices[j])))\n",
    "                        candidates.add(pair)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "hashes = df['simhash'].tolist()\n",
    "search_start_time = time.time()\n",
    "candidates = lsh_candidates(hashes, band_size=int(128/(K+1)), num_bands=K+1)\n",
    "\n",
    "found_pairs = set()\n",
    "for i, j in candidates:\n",
    "    # dist = hamming_distance(hashes[i], hashes[j])\n",
    "    dist = hashes[i].distance(hashes[j])\n",
    "    if dist <= K:\n",
    "        found_pairs.add((i, j))\n",
    "true_pairs_from_id = generate_ground_truth(df)\n",
    "true_positives = len(found_pairs.intersection(true_pairs_from_id))\n",
    "precision = true_positives / len(found_pairs) if len(found_pairs) > 0 else 0\n",
    "recall = true_positives / len(true_pairs_from_id) if len(true_pairs_from_id) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "print(f\"Ground truth pairs:{len(true_pairs_from_id)}\")\n",
    "print(f\"Total candidates found by SimHash:{len(found_pairs)}\")\n",
    "print(f\"True positives:{true_positives}\")\n",
    "print(f\"Missed:{len(true_pairs_from_id) - true_positives}\")\n",
    "print(f\"Precision:{precision:.4f}\")\n",
    "print(f\"Recall:{recall:.4f}\")\n",
    "print(f\"F1-Score:{f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
